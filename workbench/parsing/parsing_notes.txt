
Minishell is divided into two parts:
	Parsing: where we treat the user input
	Execution: where you execute what have been parsed

Minishell is a command-line interpreter that mimics the bash and its basic funcionalities:
	Shell will work only in interactive mode, meaning no scripts - the 
    executable takes no arguments, it will start up and immediately enters a    
    loop where it waits for user input;
	Run simple commands with absolute, relative path (e.g. /bin/ls, ../bin/ls);
	Run simple commands without a path (e.g. ls, cat, grep, etc.);
	Navigation through commands with up/down arrows;
	Pipes;
	Redirections (<, >, >>);
	Here-doc (<<, delimeter that displays a new prompt, reads user input until 
    reaching the delimeter, redirects user input to command input - does not 
    update history);
	Handle double quotes (""), and single quotes (''), which should escape  
    special characters, beside $ for double quotes;
	Handle environment variables ($ followed by a sequence of characters, e.g.  
    $USER or $VAR, that expand to their values);
	$?, expands to the exit status of the most recently executed foreground 
    pipeline;
	Handle signals like in bash:
		ctrl + C, displays a new prompt line; 
		ctrl + D, exits minishell;
		ctrl + \, does nothing;
	Implement the following built-ins:
		echo (option -n only);
		exit;
		env (with no options or arguments);
		export (with no options);
		unset (with no options);
		cd;
		pwd;
	NOTE: Minishell does not support \, ;, &&, || or wildcards.

LEXING

Lexing (or tokenization) is the process of breaking down an input command line into a sequence of meaningful tokens. These tokens represent commands, arguments, operators, and other special characters. Essentially, the lexer identifies what's in the input string and categorizes it, without worrying about the command's validity or meaning.
Here's a more detailed breakdown:
	Input: The lexer takes the user's input command line as input. 
	Tokenization: It reads the input character by character, identifying
    different types of tokens. 
	Token Types: Common tokens include:
	Commands: The executable program (e.g., ls, echo). 
	Arguments: Data passed to the command (e.g., ls -l). 
	Operators: Piping (|), redirection (<, >, >>), and others. 
	Special Characters: Quotes, semicolons, and other symbols. 
	Output: The lexer produces a list or stream of these tokens. 
	Purpose: This token stream is then used by the parser to build an abstract
    syntax tree (AST) that represents the structure of the command line. 
For example, the command ls -l | wc -w > output.txt would be lexed into tokens like: ls, -l, |, wc, -w, >, output.txt. 
The lexer is a crucial first step in processing user input in a Minishell implementation.

TWO THINGS TO TAKE CARE OF IN FRONT-END (which deals with user input and user interaction, like 
commands and signals)
	Commands (user input as a line/string);
	Signals (ctrl + C, etc.);

PARSING goes through two phases:
	1st, Lexical analysis/tokenization, which produces "lexems" or tokens;
	2nd, Syntax analysis (parsing the tokens);
	
TOKENS
    Commands
        Primary action or program to be executed
            'ls -l', 'ls' is the command token
    Arguments
        Additional info or parameters provided to the command
            'ls -l', '-l' is an argument token that modifies the behavior of 
            the 'ls' command
    Operators
        Special symbols that control the flow of commands
            Pipes ('|'), used to pass the output of one command as input to     
            another
            Redirection ('>', '<', '>>'), used to redirect input and output to
            and from files
    Delimeters
        Characters that separate tokens
            Spaces and newlines often serve as delimeters in shell commands
    Quotes
        Tokens that indicate that the enclosed text should be treated as a 
        single argument, even if it cointain spaces
                'echo "Hello, World!"', the quote indicates that 'Hello, 
                 World!' is a single token
    Comments
        Text that is not executed but provides info to the user or developer
        starts with a '#' and continues to the end of the line, meaning that in 
        the parsing, when it identifies a '#' character, should ignore it

    EXAMPLE
        grep "error" logfile.txt | sort -r > errors_sorted.txt # 
        identifying errors
        
        Command tokens: 'grep', 'sort'
        Arguments: '"error"', 'logfile.txt', '-r', 'errors_sorted.txt'
        Operators: '|', '>'
        Delimenters: spaces between tokens
        Comments: # identifying errors

    IMPORTANCE OF TOKENS
        Simplification
            By breaking down input into tokens, the shell can simplify the 
            process of interpreting and executing commands
        Error detection
            Tokenization helps in identifying syntax errors early in the 
            process, making it easier to provide feedback to the user
        Structured processing
            Tokens allow the parser to understand the structure of the command,
            enabling it to build an abstract representation (like an AST) for   
            execution


TOKENIZATION PROCESS (tokens.c & tokens_helpers.c)

The goal of the tokenization process is to parse a shell-like input string into a list of t_token objects, each representing a meaningful unit of the input. Each token has:
	A type (t_cat) that identifies its category (e.g., COMMAND, ARGUMENT, OPERATOR, etc.);
	A content string (t_token, *char content) that holds the actual value of the token (e.g., 
	"ls", "-l", "|", etc.).

1. Input initialization
	The tokenize_input function initializes:
		tokens: a linked list (t_list) to store the resulting tokens;
		i: a position index to traverse the input string
		expect_command: a flag to determine whether the next token is expected to be a COMMAND
		or an ARGUMENT. Depending on this flag, the behaviour will be different in the way it
		works the token
		
	Whitespace skipping
		Leading and intermediate whitespace is skipped using ft_isspace;
		If the end of the input string is reached after skipping whitespace, the loop exists.
	
	Token handling
		The token_handling function is called to process the current segment of the input 
		string;
		This function determines the type of token and delegates processing to specific helper
		funtions:
			Operators: handled by op_handling;
			Words: handled by word_handling;
			Quotes: if implemented, would be handled by quote_handling.

	Operator handling (op_handling)
		If the current character is an operator (e.g., |, <, >, >>), the function:
			Identifies the operator by checking the current and next characters;
			Extracts the operator substring using ft_substr;
			Creates a t_token of type OPERATOR using create_token;
			Adds the token to the tokens list using ft_lstadd_back.
	
	Word handling (word_handling)
		If the current segment is a word (e.g., a command or argument), the function:
			Identifies the word by iterating until a delimeter is found (e.g., whitespace,
			quotes, operators, or comments);
			Extracts the word substring using ft_substr;
			Determines the token type:
				If expect_command is 1, the token is a COMMAND;
				Otherwise, it is an ARGUMENT.
			Creates a t_token using create_token;
			Adds the token to the tokens list using ft_lstadd_back;
			Resets expect_command to 0 after processing a command.
	
	Quote handling (if implemented)
		If the current character is a quote (' or "), the function:
			Extracts the quoted string, including spaces, until the closing quote;
			Creates a t_token of type QUOTE with the quoted content;
			Add the token to the tokens list.
	
	Comment handling
		If the current character is #, the rest of the input is ignored as a comment;
		No token is created for comments.
	
	Error handling
		If any helper function fails (e.g., memory allocation issues), it returns 1, and
		tokenize_input returns NULL to indicate failure;
		If a special condition is encountered (e.g., end of input), token_handling may return
		2 to break the loop.
	
	Token list of completion
		Once the entire input string is processed, the tokens list contains all the parsed
		tokens as t_token objects;
		The function returns the tokens list.
	
		






















